# 4. Evaluation and Monitoring
## Table of Contents

## 4.1 Intoroduction to Monitoring Answer Quality
## 4.2 Evaluation and Monitoring LLMs
### Offline evaluation
* Tests that we do before deploying. E.g. Hit rate, MRR
* Cosine similiarity
* LLM as a judge
### Online evaluation
* E.g. A/B Testing, experiments, user feedback
### Monitoring
* Observing the overall health of the system
* How good the answer is

## 4.3 Offline RAG Evaluation
* Evaluate the whole RAG (previous chapter, we only evaluated the search function)
* Compare results of original answer to answers generated by LLM using cosine similarity after converting them to vectors

## 4.4 Offline RAG Evaluation: Cosine Similarity
* Calculate metrics in the test environment before deploying and applying online evaluations

### Converting dataframe to dict
```python
results_dict = df.to_dict(orient='records')
```

### Function to compute cosine similarity
```python
def compute_similarity(record):
    answer_orig = record['answer_orig']
    answer_llm = record['answer_llm']
    
    v_llm = embedding_model.encode(answer_llm)
    v_orig = embedding_model.encode(answer_orig)
    
    return v_llm.dot(v_orig)

# loop into each record and compute the similarity (dot product)
from tqdm.auto import tqdm
evaluations = []

for record in tqdm(results_dict):
    sim = compute_similarity(record)
    evaluations.append(sim)
```

